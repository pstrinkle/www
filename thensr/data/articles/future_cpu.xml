<title>CPU Speculation</title>
<author>George Liu</author>
<img>nsr_images/_articles/_smalls/s3.jpg</img>
<date>3/11/2007</date>
<area>tech</area>
<body>
<p class="body">Fusion has been the code name that people have been talking about for some time.  Fusion is a chip from AMD that will combine a CPU and a GPU.  There are a lot of advantages for an approach like this.  The integration will result in faster communication and more general speed.  Also, with a single chip replacing two chips, there are bound to be cost savings.  Though processor yields will decrease at first, system builders and chipset vendors will appreciate the savings created by less complex chipsets and fewer parts to purchase.  I can’t see this as a solution for enthusiasts, however.  With current GPUs housing more transistors than most CPUs, it would not be cost effective to develop such an enormous processor.  Also, much of the functionality present in graphics technology, such as H.264 decoding and HDMI support, can easily be outdated before a new processor generation appears.  Overall, it is an interesting idea that may be well suited for replacing integrated graphics.  It is already being tested in the form of AMD’s Geode processor, but that can hardly represent the typical x86 environment.</p>
<p class="body">Besides the typically rapid pace of GPU development and slower pace of CPU development, one more thing comes to mind.  Intel has spoken of a CPU/GPU combination, but Intel is rumored to be developing discrete GPU solutions as well.  Intel has not had the best foresight, but such a move seems to discredit the CPU/GPU theory.  Obviously, there is a future for discrete solutions.  Why, then, would companies try to develop a Fusion?  The cost savings from reducing parts and simplifying chipsets might not balance the greater development costs, lower processor yields, and forced speedup of product cycles.  Furthermore, since discrete and integrated solutions are already capable and cheap, a CPU/GPU fusion seems unnecessary.</p>
<p class="body">What seems to make sense to me is a modular approach to computing.  Currently, systems are being developed that will allow various specialized processors to be added to a system, such as the powerful Cell processor for math.  AMD’s Torrenza initiative already represents this.  But this solution likely is too specialized for most consumers.  The likely target here is the large business with thousands of servers and the money to pay for these extra processors.  Regular consumers are highly unlikely to be considered adopters of this technology.</p>
<p class="body">The thing to realize in all of this speculation is that CPU’s are designed to be extremely general purpose.  They do not excel at anything in particular due to the nature of general purpose computing.  Integrating more functionality into a CPU is an innovative idea, such as AMD’s integrated memory controller.  But designers cannot be obsessed with new functionality, since any specialization could add unnecessary cost.  Consider the VIA Eden chips.  Hardware cryptographic support and dual processor clocks are innovative ideas, but VIA chips are still notoriously slow compared to last year’s Pentiums and unattractive pricewise.  This specialization relegates VIA chips to a niche role that it has so far been unable to overcome.</p>
<p class="body">In the near future, one can expect faster interconnects, higher clock speeds, and more transistors.  Processor evolution guarantees this and proof can be seen in Intel’s Common System Link or AMD’s new Barcelona chip.  There is room for CPU/GPU combinations and we will see those in the future.  One has to wonder, though, when a true revolution will appear in the CPU scene.  X86 and its evolution into 64-bit surely won’t dominate forever, which brings us to the next question: what will be next?</p>
</body>
